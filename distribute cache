key stack:
cassaandra
zoo keeper
memcache
redis
kafka


CAP Theorem:
	consistent-availability-partition tolerent 
	you have only have two out of three!
	you can not available, so don't tell the wrong data. 
	you can send some data, but could be wrong. 
	or you give up partition
	
distributed computation:
	hadoop:
		mapreduce api
		mapreduce job management. 
		distributed file system. 
	Spark:

distributed cache:
	 scalability
	 availablity 
	 performance, 
	 durablity
	 
	 how to design:
		1st, single computer cache, -> LRU cache. 
		2nd , multiple server, hash function to split data to different zone. 
		3rd , how to re-direct request to cache server, MOD, but scale will bring down all existing cache. 
		4rd, consistent hash. circle!
		which component is master the hash? -> cache client.which sit insode of service server when send request.  
		cache client can use binary to searach which cache server holds data. 
		cache cliet use TCP or even UDp to talk to cache server
		
		how to config? 1)via config file. (most simple approach), it can be stored  in a shared location, and pulls few mins. 
		               2) create a cache service, use heartbeat to talk and register available cache server. ( zookeeper)
					   
		by this design we have scalbility, since we add more shard cache server, and we have performance since search is via hash and communication is via TCP or UDp, 
		but availablity is not there, any cache server fails, will result a mssaive cache miss. 
		

		the answer is data replication, every cache master follows a slave master. 
		this deal with hot shard. 
		
		for cache, performance and availablity is more important compre to consistency. 
		replication async will bring inconsistency. 
		client can have different cache server list. we can make sure all client share single view of cache server list, but with trade off to higher latency. 
		
		matrix to measure cache:
		latency, number of cache hit and miss, cpu and ram utilization on cache host, network I/O, 
		
3. Cache Invalidation
If the data is modified in the database, it should be invalidated in the cache, if not, this can cause inconsistent application behavior. There are majorly three kinds of caching systems: Write-through cache, Write-around cache, Write-back cache.

Write through cache: Where writes go through the cache and write is confirmed as success only if writes to DB and the cache BOTH succeed. Pro: Fast retrieval, complete data consistency, robust to system disruptions.
Con: Higher latency for write operations.

Write around cache: Where write directly goes to the DB, bypassing the cache.
Pro: This may reduce latency.
Con: However, it increases cache misses because the cache system reads the information from DB in case of a cache miss. As a result of it, this can lead to higher read latency in case of applications that write and re-read the information quickly. Read must happen from slower back-end storage and experience higher latency.




key-value cache

functional:
	get
		hit
		miss
	set
	
non functional:
	scalable
	high avilability
	high performance
	consistency?
	
Things to consider:
	high traffic spike
	load not evenly distribute
	low latency
	10 billion queries per month
	assume each request 10kb dataload, 5 kb header/meata data. 
	that 15 * 10bill/30 = 5bill kb/day = 5mill mb/day = 200 000mb/hr = 3500mb/min
	
	
	
if get :
	client send request. 
	request hit web server. 
	web server hit query api. 
	query parse the data, stripe the header, extract the request dataload, ->hash value -> checks memory cache
	
	if cache hit
	if data is not expired in memory, return content
	
	if get cache miss:
	query service request data from database 

if set:
	


module design

	
			request
		cache_client build inside web service, each client has the same config and knows which cluster to point
		cache client service( S3, or zookeeper)
	cache cluster(a), b, c, d
	
	database, master-replica-replica-replica

cache_clist-> how to maintain a cache cluster list, 1) keep a config file 3) keep the config in a public storage, like s3, 3) discovery service like zookeeper

inside each cluster,  each master has a replica to share load and can be promoted to master. 
					  all master nodes talk to each other, under management of configuration service

when data is updated to cache and database 
write through
write back




Write back cache: Where the write is directly done to the caching layer and the write is confirmed as soon as the write to the cache completes. The cache then asynchronously syncs this write to the DB.
Pro: This would lead to a really quick write latency and high write throughput for the write-intensive applications.
Con: However, there is a risk of losing the data in case the caching layer dies because the only single copy of the written data is in the cache. We can improve this by having more than one replica acknowledging the write in the cache.

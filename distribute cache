key stack:
cassaandra
zoo keeper
memcache
redis
kafka


CAP Theorem:
	consistent-availability-partition tolerent 
	you have only have two out of three!
	you can not available, so don't tell the wrong data. 
	you can send some data, but could be wrong. 
	or you give up partition
	
distributed computation:
	hadoop:
		mapreduce api
		mapreduce job management. 
		distributed file system. 
	Spark:

distributed cache:
	 scalability
	 availablity 
	 performance, 
	 durablity
	 
	 how to design:
		1st, single computer cache, -> LRU cache. 
		2nd , multiple server, hash function to split data to different zone. 
		3rd , how to re-direct request to cache server, MOD, but scale will bring down all existing cache. 
		4rd, consistent hash. circle!
		which component is master the hash? -> cache client.which sit insode of service server when send request.  
		cache client can use binary to searach which cache server holds data. 
		cache cliet use TCP or even UDp to talk to cache server
		
		how to config? 1)via config file. (most simple approach), it can be stored  in a shared location, and pulls few mins. 
		               2) create a cache service, use heartbeat to talk and register available cache server. ( zookeeper)
					   
		by this design we have scalbility, since we add more shard cache server, and we have performance since search is via hash and communication is via TCP or UDp, 
		but availablity is not there, any cache server fails, will result a mssaive cache miss. 
		

		the answer is data replication, every cache master follows a slave master. 
		this deal with hot shard. 
		
		for cache, performance and availablity is more important compre to consistency. 
		replication async will bring inconsistency. 
		client can have different cache server list. we can make sure all client share single view of cache server list, but with trade off to higher latency. 
		
		matrix to measure cache:
		latency, number of cache hit and miss, cpu and ram utilization on cache host, network I/O, 
		
3. Cache Invalidation
If the data is modified in the database, it should be invalidated in the cache, if not, this can cause inconsistent application behavior. There are majorly three kinds of caching systems: Write-through cache, Write-around cache, Write-back cache.

Write through cache: Where writes go through the cache and write is confirmed as success only if writes to DB and the cache BOTH succeed. Pro: Fast retrieval, complete data consistency, robust to system disruptions.
Con: Higher latency for write operations.

Write around cache: Where write directly goes to the DB, bypassing the cache.
Pro: This may reduce latency.
Con: However, it increases cache misses because the cache system reads the information from DB in case of a cache miss. As a result of it, this can lead to higher read latency in case of applications that write and re-read the information quickly. Read must happen from slower back-end storage and experience higher latency.

Write back cache: Where the write is directly done to the caching layer and the write is confirmed as soon as the write to the cache completes. The cache then asynchronously syncs this write to the DB.
Pro: This would lead to a really quick write latency and high write throughput for the write-intensive applications.
Con: However, there is a risk of losing the data in case the caching layer dies because the only single copy of the written data is in the cache. We can improve this by having more than one replica acknowledging the write in the cache.

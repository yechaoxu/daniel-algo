Load balancer:
             Dcan use rounding robbin, hash the ip. 
             https://www.acodersjourney.com/system-design-interview-load-balancing/
              Load balancer:

              Layer4
              good
              only look at the ip, not the data. 
              simpler load balancing, efficient. don't access the data layer. 
              minimise tcp connection


              bad
              no smart load balancing
              sticky per segment.
              no caching

              Layer 7
              need to terminate tcp connection and create a new one
              smart routing by read data layer. 
              can cache. 

cache:
      how to manage cache is called cache policy ( LRU)
      cache miss
      cache size matter. if it is too small it will hurt performance. 
      cache consistency:
                        write through , means write operation go through cache before , before goes to database 
                        write back.      means write operation to cache, then to database asynchronously. 


Database Index:
        In database management systems, a reverse key index strategy reverses the key value before entering it in the index.1 E.g., 
        the value 24538 becomes 83542 in the index. Reversing the key value is particularly useful for indexing data such as sequence numbers,
        where each new key value is greater than the prior value, i.e., values monotonically increase.
        Reverse key indexes have become particularly important in high volume transaction processing systems because they reduce contention for index blocks
        
Consistent hashing
        by simple approach, hash val % server number will evenly distribute the work load, but this bring an issue when one server is down or newly add. All hashing value will change
        and need to redo. thus we introduce the consisten hashing, it is a ring shape which each server take a node. when a request comes, it get the hash value,
        land on the ring at 360 degree, and go clock wise to land on the 1st available server. 
        if a new server is added, it will only add to existing ring and very minial hash value will be impacted. ( only the one in the ring cycle before this new node)
        to consider load sharing, a virtual node can be used, in practice it is widely used. for example for a 4 node system , each covers 90 degree, a new node added and share load with 
        4th node, each with 45 degree, the new node will create a virtual node covers with 1st node as well. so the load can be evenly distributed. 
        
 Message Queue/task queue.
        it takes the tasks, distribute to each worker/server, if the server is dead(no heartbeat), it will re-assign to another node. 
        rabbit message queue. 


Consistent hashing
Data replication
Total order broadcast
Write ahead logging
Two phase locking
Isolation levels (read uncomitted, read comitted, repeatable read, serializable)
Quad trees (GeoHashin)
Bloom filters




zookeeper, 
  it does the configuration of multiple nodes, detecs master/salve nodes, distributed, each node contains all info , can do quorum check 

kafka 
  it can acts as a message exchange system, producer post msg to the broker, msg is seggrate based on topic. 
  broker will do parition to split the msg ( parition 1/2)
  consumer form a group, two (a,b) in one group, this group will share load from all paritions( a read from 1, b read from 2),which you are reading this topic as queue
  if you want msg to broadcast, you can assign each consumer in a unique group so each consumer will read all msgs from that topic. 

vs rabbitMQ

cassandra 
S3
aws load balancer ELB
messsage:   kafka
dadta process: Spark, 
Storage: Cassandra, 
object storage: S3. 
RabbitMQ.
cache:  Redis, memcache
Service discovery: zookeeper


how to identify bottleneck:
  load test        we want to know how system can scale the 
  stress test   
  unit test   


long pull vs websocket
What is Long polling ?
A variation of the traditional polling technique and allows emulation of an information push from a server to a client. With long polling, the client requests information from the server in a similar way to a normal poll.

If the server does not have any information available for the client, instead of sending an empty response, the server holds the request and waits for some information to be available.
Once the information becomes available (or after a suitable timeout), a complete response is sent to the client. The client will normally then immediately re-request information from the server, so that the server will almost always have an available waiting request that it can use to deliver data in response to an event.

In a web/AJAX context, long polling is also known as Comet programming.	
		
		
WebSockets provide a persistent connection between a client and server that both parties can use to start sending data at any time.

The client establishes a WebSocket connection through a process known as the WebSocket handshake. This process starts with the client sending a regular HTTP request to the server.
An Upgrade header is included in this request that informs the server that the client wishes to establish a WebSocket connection.
Conclusion :

If there is a need of Real time communication you can very well opt for websockets .

But in Long Polling :

A connection is held open between the web client and the web server so that when the server has new information it can push it to the client. 
That request is then finished. A new request is then made between the client and the server and then wait for another update from the server. 
The same TCP connection is generally open persistently throughout multiple requests due to HTTP/1.1 keep-alives.
